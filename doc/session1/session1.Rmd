---
title: 'Machine Learning class: session 1'
author: "Kevin Vervier"
date: "August 1, 2016"
output: html_document
---

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

```{r ,message=FALSE, echo=FALSE}
#package installation block
library(devtools)
#nice pca
library(ggbiplot)
```


## Learn to know your data

First, one needs to load the data set of interest. Usually, data $X$ is made of <b><span style="color:#6495ED">observations</span></b> in row and <b><span style="color:#6495ED">attributes</span></b> in column.
Given the format of your data file, you may choose different `read.` functions (e.g., `read.csv`,`read.table`,...).

<b>Important</b>: for the whole tutorial, we assume that the first column of the data is the response variable ($Y$).

For demonstration purpose, we will use a modified version of a very popular data set: Edgar Anderson's Iris Data (`iris`).

```{r }
data <- read.csv('data/iris.csv')
head(data,10)
```

This version of the dataset is based on a collection of 5 measurements (petal/sepal length/width and stem length) obtained on 151 iris flowers, belonging to 3 different species (*setosa, versicolor, virginica*). 

Here, one question could be: 

<b> Given those measurements on a new flower, is it possible to predict its corresponding species ? </b>


### Descriptive statistics

* Describe input variables covariation with the outcome, to get a sense of what could be the <b><span style="color:#6495ED">important predictors</span></b> and correlation.

```{r }
#'pairs' function does scatterplots for each pair of variables and use labels for color.
pairs(data[2:ncol(data)], main = "Correlation between variables", pch = 21, bg = c("red", "green3", "blue")[unclass(data[,1])])
```

* Here, it seems that the `stem_length` argument is not correlated with iris species. We could decide to exclude this feature from the study.

```{r }
data$stem_length = NULL
```

* It is generally a good idea to check for <b><span style="color:#6495ED">outliers</span></b> in the data (due to typo, technical noise,...) via Principal Component Analysis (<b><span style="color:#6495ED">PCA</span></b>). 
    * This technique is a simple way to represent high-dimensional data in only 2 dimensions.
    * PCA looks for the <b><span style="color:#6495ED"> best linear combination of variables</span></b> to define new variables, encompassing as much variance as possible.
    
```{r }
#separate species name (labels)
labels <- data[,1]
# keep input variables only
obs <- data[,-1]

# 'prcomp' is the Principal Component function in R.
res.pca <- prcomp(obs) 
# See first 2 PCs
print(res.pca$rotation[,1:2])
```
This table contains the Principal Components (PCs) and their coordinates with the original variables.

```{r }
# Plot the 2 first PCs
# One arrow corresponds to one variable and how it is used for PC1 (x) and PC2 (y).
ggbiplot(res.pca , obs.scale = 1, groups = labels)

```

It seems that we have an outlier belonging to Setosa species. It is preferable to keep it apart from the analysis, while investigating the reason of this measure. 

```{r }
data = data[-1,]
```
NB: Outlier detection is a whole field of research and there is more advanced techniques to test wether or not an observation is an outlier.

This is how it looks <b> after outlier filtering </b>.
```{r ,message=FALSE, echo=FALSE }
#take species name apart
labels <- data[,1]
# keep input variables only
obs <- data[,-1]

# 'prcomp' is the Principal Component function in R.
res.pca <- prcomp(obs) 

# Plot the 2 first PCs
ggbiplot(res.pca , obs.scale = 1, groups = labels)

```

Now that the dataset has been cleaned, it would be easier for us to train our first machine learning model.
We may want to save the pre-processed data set in order to load it easily in the next sessions.

```{r}
write.csv(data,file='data/iris_processed.csv',quote=FALSE,row.names = FALSE)
```
---

## Introduction to Machine learning

* Take the <b><span style="color:#6495ED">observed</span></b> data, try to find some rules, regularities in the data in order to make predictions for the <b><span style="color:#6495ED">unseen</span></b> observations.

* Machine learning arose as a subfield of Artificial Intelligence and has a greater emphasis on large scale applications and prediction accuracy.
* Known Machine Learning <b><span style="color:#6495ED"> trade-offs</span></b>:
    * Prediction accuracy versus interpretability: linear models are easy to interpret, but usually are not the best performers
    * Parsimony versus black-box: number of predictors in the model
    * Good fit versus overfitting: overfitting the training data decreases the generalization of the model

## Supervised learning

It refers to the case where your training data are labelled, or have an outcome value already measured.

NB: unsupervised learning is another field where there is not label on the data $\Rightarrow$ clustering.


Applications of supervised learning: 

* Decide which movie you might want to watch next on Netflix,
* Identify the numbers in a handwritten zip code,
* Estimate gene expression level based on multiple SNPs alleles count ([geno2expr](https://github.com/kevinVervier/geno2expr-ML) approach),
* Classify an individual into one of several diagnosis classes, based on multiple phenotypic measurements.

(*Quizz*: could you cite other examples (outside of your research field) ?) 

Learning framework: 

* Input/predictor/feature/covariate measurement vector $X$
* Output/response/target measurement $Y$
    * In a <b><span style="color:#6495ED">regression</span></b> problem, $Y$ is a quantitative variable.
    * In a <b><span style="color:#6495ED">classification</span></b> problem, $Y$ takes values in a finite set.
    
(*Quizz*: based on thoses definitions, could you `classify` the previous examples ?) 
    


Based on <b><span style="color:#6495ED">training data</span></b> which are pairs $(xi,yi)$, we are interested in:

1. accurately predicting unseen cases, 
2. assessing the quality of our predictions,
3. understanding which inputs affect the output.

---


## Linear Regression

* Simple approach for supervised learning that assumes a linear dependence between $Y$ and $X$, meaning that there exists constant weights $\beta_i$ for each predictor, such as $$Y = \beta_0 + \beta_1\times X_1 + ... + \epsilon,$$

where $\beta_0$ represents the bias in the data, and $\epsilon$ is a random noise that is not captured by the predictors.

* This approach works well when $Y$ is a numeric variable, but we will see an alternative approach for qualitative output.
* Although it may seem overly simplistic, linear regression is extremely useful both conceptually and practically.
* Parameters estimation by Residual Sum of Squares (RSS): minimizing residuals between ground truth and fitted values.

![Source: http://gerardnico.com/wiki/data_mining/residual](pic/statistics_residual.jpg)

 Here, we will change a bit our data set, because the original Iris problem is not a regression one, but a classification problem (see later).
 
 Let say that we are interested in predicting the Sepal length (column 1) of a flower, given the Sepal Width.

```{r }

fit <- lm(sepal_length ~ sepal_width, data = obs) # 'lm' stands for Linear Model.
summary(fit)

```

* To test if there is a significant relationship between one predictor and the outcome, we could compute a statistical test and apply a threshold on the p-value.
    * Fortunately, it is already done in `lm` $\Rightarrow$ look for the stars in the <b> pvalue </b> column (right).
* Accuracy of the fit is measured using $R^2$, also provided by `lm`. It can help for model comparison.
```{r }
# Here, we will consider the 3 variables as predictors.
fit <- lm(sepal_length ~ ., data = obs) # '.' stands for all the other variables 
summary(fit)

```

* *Advanced*: In its simplest version, linear regression does not test correlation between predictors, which could induce confusion when interpreting coefficients (here, Petal length and width).
    * In case of correlation between predictors, it is usually a good idea to consider _interactions_ between features as additional features.
    * There is also non-linear regression methods (e.g. splines or polynomial).

<b> Congrats ! You just trained your first models ! </b>
but...

* What about the trade-offs we discussed earlier, especially the generalization ? 
* How to evaluate your model performances on future data ?
* How to compare your model performances with other models ?

![<b>Source</b>: An Introduction to Statistical Learning with Applications in R (Fig 2.10). <b>Left</b>:
Data simulated from a function $f$ (almost linear), shown in black. Three estimates of $f$ are shown: the linear regression line (orange curve), and two smoothing spline fits (blue and green curves). <b>Right</b>: Training MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed line). Squares represent the training and test MSEs for the three fits shown in the left-hand panel.](pic/overfit.png)

## Homework 1

Play around `iris` data and linear regression (hints):

* Try different subsets of variables,
* Try to include the `stem_length` variable and check if we were right to exclude it,
* Try to use _interactions_ between correlated variables with formula like `lm(sepal_length ~ sepal_width + sepal_width*petal_width, data =obs)` (`*` means interaction between features),
* Try to fit splines (non-linear), using  `loess` function (which has a very similar synthax that `lm`).
* ...

#Ressources

[(1) Hastie and Tibshirani Online course](http://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/)

[An Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf)

[Introduction to ML with knn applied on iris data](http://www.datacamp.com/community/tutorials/machine-learning-in-r)


