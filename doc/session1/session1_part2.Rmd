---
title: 'Machine Learning class: session 1 part 2'
author: "Kevin Vervier"
date: "August 3, 2016"
output: html_document
---

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

---


```{r ,message=FALSE, echo=FALSE}
#nice pca
library(ggplot2)
# Embedded feature selection (ridge, Lasso, Elastic-Net)
library(glmnet)
```



# Session 1 debriefing

* Data preparation: non-discriminative variable , outliers, missing data, ...
* Supervised Learning: regression and classification
* Linear Regression
* Overfitting

---

First, we need to load the data we processed during the last session:
```{r }
data <- read.csv('data/iris_processed.csv')
obs <- data[,-1]
```

## Model Selection and Regularization

In this section, we will study how to get generalizable models, optimize method parameters and how to evaluate/compare different models.

### Train/Validation/Test sets

* As we saw in the previous picture, it is important to <b> not use the training error </b> as a performance measure (<b><span style="color:#6495ED">overfitting</span></b>)
* The test error is the average error obtained on a new observations set, not used in the training step.
* Hold-out a subset of the training observations from the fitting process and estimate test error on it.
* Here is how to use Train/Validation/Test rule of thumb:
    * Consider 60% of your data for fitting model
    * 20% of the remaining data are used as validation set, especially if there is parameter optimization needed
    * the last 20% of the data are the test/hold-out data, only used when the final model is fitted and estimates test error
    * Main drawback: **need several repeats**, because performances usually depend of the considered random split.
   
![Source: http://images.slideplayer.com/17/5305651/slides/slide_14.jpg](pic/train_val_test.jpg) 
    
    
```{r }

# Code for a train/validation/test split and evaluation of two models

trainValTestLM <-function(seed = 42){
  set.seed(seed) # fix random seed for reproducible results
  # Generate random index
  idx = sample(c('train','validation','test'),nrow(obs),replace=TRUE,prob = c(0.6,0.2,0.2))
  # Sanity check
  #table(idx)

  # split X
  X.train = obs[which(idx == 'train'),]
  X.val = obs[which(idx == 'validation'),]
  X.test = obs[which(idx == 'test'),]

  # Fit model with only sepal width predictor
  fit1 <- lm(sepal_length ~ sepal_width, data = X.train)
  # Fit model with all predictors
  fit2 <- lm(sepal_length ~ ., data = X.train)

  
  # Evaluate both models on training (BAD)
  preds.train1 = predict(fit1,X.train)
  preds.train2 = predict(fit2,X.train)

  # Evaluate both models on validation
  preds1 = predict(fit1,X.val)
  preds2 = predict(fit2,X.val)
  
  # One performance measure is the Mean Square Error (MSE) between actual and predicted values:
  
  #### train ####
  # Model 1
  mse.train1 = mean((X.train[,1] - preds.train1)^2)
  # Model 2
  mse.train2 = mean((X.train[,1] - preds.train2)^2)
  
  #### validation ####
  # Model 1
  mse1 = mean((X.val[,1] - preds1)^2)
  # Model 2
  mse2 = mean((X.val[,1] - preds2)^2)

  
  #select best model
  if( mse1 < mse2 ){
    best.model = fit1
    best = 1
  }else{
    best.model = fit2
    best = 2
  }
  
  # Do test prediction
  preds.test = predict(best.model,X.test)
  # Get MSE for the test set
  mse.test = mean((X.test[,1] - preds.test)^2)
  #return informations 
  return(list('best.model' = best, 'model1' = fit1, 'model2' = fit2, 'train.perfs.1' = mse.train1, 'train.perfs.2' = mse.train2, 'val.perfs.1' = mse1, 'val.perfs.2' = mse2,'test.perfs' = mse.test))
  
}

#Call function for one random seed
results = trainValTestLM(seed = 42)
```

```{r, echo = FALSE}
cat('Model 1 performance on Train set:',results$train.perfs.1,'\n')
cat('Model 1 performance on Validation set:',results$val.perfs.1,'\n')

cat('Model 2 performance on Train set:',results$train.perfs.2,'\n')
cat('Model 2 performance on Validation set:',results$val.perfs.2,'\n')


cat('Best model (', results$best.model,') performance on Test set:',results$test.perfs,'\n')

```

```{r}
#Call function for a different seed
results = trainValTestLM(seed = 89)
```

```{r, echo = FALSE}
cat('Model 1 performance on Train set:',results$train.perfs.1,'\n')
cat('Model 1 performance on Validation set:',results$val.perfs.1,'\n')

cat('Model 2 performance on Train set:',results$train.perfs.2,'\n')
cat('Model 2 performance on Validation set:',results$val.perfs.2,'\n')


cat('Best model (', results$best.model,') performance on Test set:',results$test.perfs,'\n')

```


Again, you may need to <b> repeat this process </b> (by changing the random seed) to assess model variability as a function of the random split (~50-100X), before applying on test.

### Cross-validation

* An alternative: $K$-fold cross-validation (e.g. $K$ = 5 or 10 or LOO)
    * randomly divide the data into $K$ equal-sized parts.
    * loop for $k$ in 1 to $K$: leave out part $k$ and fit the model to the other $K-1$ combined parts
    * obtain predictions on the $k$-th part
    * get cross-validation error estimation by combining all predictions
    * LOO stands for 'Leave-One-Out', where $K$ = `nrow(data)`.
    
![Source: http://bugra.github.io/work/notes/2014-11-22/an-introduction-to-supervised-learning-scikit-learn/](pic/cv.png) 

```{r }
# Code for a cross-validation evaluation
set.seed(42) # fix random seed for reproducible results
# Number of folds/rounds
nfolds = 10
# Generate random index
idx = sample(1:nfolds,nrow(obs),replace=TRUE)
# Sanity check
table(idx)

# Initiate vectors for storing predictions from 2 models
preds1 = rep(0,nrow(obs))
preds2 = rep(0,nrow(obs))

# Loop/Rotation over the different folds
for(fold in 1:nfolds){
  #get index of test fold
  test.idx = which(idx == fold)
  # split train/test
  X.test = obs[test.idx,]
  X.train = obs[-test.idx,]
  # Fit model with only sepal width predictor
  fit1 <- lm(sepal_length ~ sepal_width, data = X.train)
  # Fit model with all predictors
  fit2 <- lm(sepal_length ~ ., data = X.train)

  # Evaluate both models on validation
  preds1[test.idx] = predict(fit1,X.test)
  preds2[test.idx] = predict(fit2,X.test)
}

# In terms of Mean Square Error (MSE) ~ average of residuals
# Model 1
mean((obs[,1] - preds1)^2)
# Model 2
mean((obs[,1] - preds2)^2)

```

By using one of these approaches, you avoid overfitting and get a fair way to compare model at the same time.

---

### Feature selection and Regularization

We already checked with `iris` linear regression example that it can be cumbersome to try every possible combination of features in your model.

Good news! There is more systematic ways to select the most meaningful features in your model:

* <b> Filter-based </b> selection: iteratively select the best feature subset, implying that some features are not relevant (basically, follow the stars!)
    * Backward selection starts with all the variables and remove one feature at a time,
    * Forward selection starts with empty model (intercept) and add one feature at a time.

* <b> Embedded/Shrinkage </b> approaches: feature selection during the training step, by regularizing/constraining coefficient estimates towards zero.
    * Idea: minimize a linear regression with additional constraints on the model
    $$ \min_w \mbox{RSS}(Xw,Y) + \lambda\Omega(w),$$
    where $\Omega(w)$ is a function that has higher value for $w$ not respecting the constraints,
    and $\lambda$ is a constant that controls the trade-off between the fit and the constraints.
    * Ridge regression ($L2$): keep all coefficients in the same range, but no selection ($\Omega(w) = \sum_i w_i^2$)
    * Lasso ($L1$): force some of the coefficients to be equal to 0 $\Rightarrow$ sparse model ($\Omega(w) = \sum |w_i|$)
    * Elastic-Net ($L1 + L2$): combination of ridge and Lasso, leading to sparse model and better handling correlated predictors $\Rightarrow$ $\Omega(w) = (1-\alpha)\times\mbox{Ridge} + \alpha\times\mbox{Lasso}$.
    * *Advanced*: regularization is a research field, where one creates new set of constraints, based on known structure in the data (e.g., Group Lasso)

![<b>Source</b>: An Introduction to Statistical Learning with Applications in R. ](pic/ridge_lasso.png)

(*Quizz*: shape of the elastic-net "ball" for $\alpha=0.5$?)


Here is an example of cross-validation evaluation that deals with feature selection <b> and </b> parameter optimization (here, $\alpha$ and $\lambda$), using `glmnet` package.

```{r }
# Code for a cross-validation evaluation
set.seed(42) # fix random seed for reproducible results
# Number of folds/rounds
nfolds = 10
# Generate random index
idx = sample(1:nfolds,nrow(obs),replace=TRUE) 

#define alpha parameter
alphas <- seq(from = 0, to = 1, by = 0.01)

#define output
mses <- rep(0,length(alphas))

#loop over alpha values
for(i in 1:length(alphas)){
  # cv.glmnet function does the CV loop for the different folds
  cvfits <- cv.glmnet(x=as.matrix(obs[,-1]), y=obs[,1], alpha=alphas[i], nfolds=nfolds,foldid=idx) 
  # for a given alpha, lambda is optimized inside cv.glmnet
  preds.regul = predict(cvfits, newx = as.matrix(obs[,-1]), s = "lambda.min")
  mses[i] <- mean((obs[,1] - preds.regul)^2)
}
this <- data.frame(mse=mses, alpha=alphas)

plot1 <- ggplot(this, aes(x=alpha, y=mse)) +
  geom_line() +
  ylab("CV mean squared error") +
  xlab("alpha parameter") +
  ggtitle("model error of highest performing regularized elastic-net
           regression as a function of alpha parameter") + 
       #add kitchen sink
          geom_hline(aes(yintercept=mean((obs[,1] - preds2)^2)),
                       size=0.2,linetype = 2,colour='dodgerblue4') +
    geom_text(aes(x = 0.7, y = mean((obs[,1] - preds2)^2)+0.01*mean(range(this$mse)) , label = "Linear model (lambda = 0)"), colour = "dodgerblue4") + theme(legend.position="none")
plot1

#get best model among alpha-range
best.alpha = alphas[which.min(mses)]
#retrain model with best_alpha
cvfits <- cv.glmnet(x=as.matrix(obs[,-1]), y=obs[,1], alpha=best.alpha, nfolds=nfolds,foldid=idx)

# get all the coefficients as a function of lambda
coefs = coef(cvfits, s=cvfits$lambda)[-1,]

par(mar=c(4.5,4.5,1,4))
plot(log10(cvfits$lambda),coefs[1,],type='l',ylim = range(coefs),xlab = 'log(Lambda)',ylab='coefficients')
lines(log10(cvfits$lambda),coefs[2,],col='red')
lines(log10(cvfits$lambda),coefs[3,],col='green')
# add variable names
axis(2, at=coefs[,ncol(coefs)],line=-.5,label=rownames(coefs),las=1,tick=FALSE, cex.axis=0.5) 
# add lambda min
abline(v=log10(cvfits$lambda.min),lty = 2)

# print best parameters pair
cat('Best alpha:',best.alpha,' and best lambda:',cvfits$lambda.min,'\n')
```
Given the small $\lambda$, there is little hope to observe the shrinkage effect, here.

```{r }
#get preds for the best (alpha,lambda) pair
preds.regul = predict(cvfits, newx = as.matrix(obs[,-1]), s = "lambda.min")
# In terms of Mean Square Error (MSE)
# Linear Regression with all variables
mean((obs[,1] - preds2)^2)
# Regularized Model
mean((obs[,1] - preds.regul)^2)

# Check the best Elastic-Net model
coef.regul = coef(cvfits, s = "lambda.min")

# Standard Linear Regression model
coef.lm = coef(fit2)

cbind(coef.regul,coef.lm)
```
This model is slightly different from the Linear Model one, but different enough in terms of performance.

Sometimes regularization is not largely outperforming standard Linear Regression, but it is worth to try it, because, **theoretically it will never do worse**. (*Quizz*: why ?)



## Homework 2

When fitting the model for estimating sepal length, we did not use the iris species as a cofactor for the linear model. But is it useful to include it ?

* Plot the sepal length as a function of iris species (*Hint*: boxplot),
* Add dummy (binary) variables to iris dataset that represent species (*Hint*: 2 of them are enough),
* Fit linear model on this new data set (*Hint*: using cross-validation and regularization).
* Demonstrate if the species is an important cofactor to include or not


#Ressources

[(1) Hastie and Tibshirani Online course](http://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/)

[An Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf)

[Introduction to ML with knn applied on iris data](http://www.datacamp.com/community/tutorials/machine-learning-in-r)

