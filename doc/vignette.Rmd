---
title: "Machine Learning tutorial"
author: "Kevin Vervier"
date: "July 26, 2016"
output: html_document
---


```{r ,message=FALSE, echo=FALSE}
#package installation block
if("devtools" %in% rownames(installed.packages()) == FALSE) {install.packages("devtools")}
library(devtools)
#nice pca
if("ggbiplot" %in% rownames(installed.packages()) == FALSE) {install_github("ggbiplot", "vqv")}
library(ggbiplot)
# Embedded feature selection (ridge, Lasso, Elastic-Net)
if("glmnet" %in% rownames(installed.packages()) == FALSE) {install.packages("glmnet")}
library(glmnet)
```

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

# Session 1: Introduction to Statistics and Machine Learning

- Statistics: inference to obtain valid conclusions from data under various assumptions
- Computer Science: efficient algorithm development, data manipulation
- Intersection $\Rightarrow$ Machine learning: takes the <b><span style="color:#6495ED">past</span></b> data, tries to find some rules, regularities in the data in order to make predictions for the <b><span style="color:#6495ED">future</span></b> observations.
- (*Quizz*) Could you cite examples of Machine Learning applications (outside of your research field)?

## Learn to know your data

First, one needs to load the data set to study. Usually, there is <b><span style="color:#6495ED">observations</span></b> in row and <b><span style="color:#6495ED">attributes</span></b> in column.
Given the format of your data file, you may choose different `read.` functions (e.g., `read.csv`,`read.table`,...).

<b>Important</b>: for the whole tutorial, we assume that the first column of the data is the response variable.

For demonstration purpose, we will use a modified version of a very popular data set: Edgar Anderson's Iris Data (`iris`).

This version of the dataset is based on a collection of 5 measurements (petal/sepal length/width and stem length) obtained on 151 iris flowers, belonging to 3 different species (*setosa, versicolor, virginica*). 

Here, one question could be: 

<b> Given those measurements on a new flower, is it possible to predict its corresponding species ? </b>

```{r }
x <- read.csv('../data/iris.csv')
head(x,10)
```
### Descriptive statistics


* Check for predictor with a too <b><span style="color:#6495ED">small variance</span></b>, that might not be helpful.

```{r }
summary(x)
```

* Describe variables variation and univariate analysis to get a sense of what could be the <b><span style="color:#6495ED">important predictors</span></b> and correlation.

```{r }
#'pairs' function does scatterplots for each pair of variables and use labels for color.
pairs(x[2:ncol(x)], main = "Correlation between variables", pch = 21, bg = c("red", "green3", "blue")[unclass(x[,1])])

# you could also use 'cor' function to compute the correlation matrix
cor(x[,2:ncol(x)])
```

* Here, it seems that the `stem_length` argument is not correlated with iris species. We could decide to exclude this feature from the study.

```{r }
x$stem_length = NULL
```

* It is generally a good idea to check for <b><span style="color:#6495ED">outliers</span></b> in the data (due to typo, technical noise,...) via Principal Component Analysis (<b><span style="color:#6495ED">PCA</span></b>). 
    * This technique is a simple way to represent high-dimensional data in only 2 dimensions.
    * PCA looks for the <b><span style="color:#6495ED"> best linear combination of variables</span></b> to define new variables, encompassing as much variance as possible.
    * In general and to keep things comparable, it is a good idea to <b> scale </b> each variable to be centered in <b> 0</b> and with a variance of <b> 1 </b>.

    
```{r }
#take species name apart
labels <- x[,1]
# keep input variables only
obs <- x[,-1]
# scale data (not mandatory)
obs = scale(obs,center= TRUE, scale= TRUE)
obs = data.frame(obs) # convert scaled object to data.frame for convenience

# 'prcomp' is the Principal Component function in R.
res.pca <- prcomp(obs) 
# See PCA results in summary
print(res.pca)

# Plot the 2 first PCs
g <- ggbiplot(res.pca , obs.scale = 1, var.scale = 1, 
              groups = labels)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal', 
               legend.position = 'top')
print(g)
```

It seems that we have an outlier belonging to Setosa species. It is preferable to keep it apart from the analysis, while investigating the reason of this measure. 

```{r }
x = x[-1,]
```
NB: Outlier detection is a whole field of research and there is more advanced techniques to test wether or not an observation is an outlier.

This is how it looks <b> after outlier filtering </b>.
```{r ,message=FALSE, echo=FALSE }
#take species name apart
labels <- x[,1]
# keep input variables only
obs <- x[,-1]
# scale data (not mandatory)
obs = scale(obs,center= TRUE, scale= TRUE)
obs = data.frame(obs) # convert scaled object to data.frame for convenience

# 'prcomp' is the Principal Component function in R.
res.pca <- prcomp(obs) 

# Plot the 2 first PCs
g <- ggbiplot(res.pca , obs.scale = 1, var.scale = 1, 
              groups = labels)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal', 
               legend.position = 'top')
print(g)
```

Now that the dataset has been cleaned, it would be easier for us to train our first machine learning model.

## Statistical learning - Machine learning

* Machine learning arose as a subfield of Artificial Intelligence and has a greater emphasis on large scale applications and prediction accuracy.
* Statistical Learning arose as a subfield of Statistics and emphasizes models and their interpretability, and precision and uncertainty.
* More and more accepted that those terms refer to the same thing.
* Known Machine Learning <b><span style="color:#6495ED"> trade-offs</span></b>:
    * Prediction accuracy versus interpretability: linear models are easy to interpret, but usually are not the best performers
    * Good fit versus overfitting: overfitting the training data decreases the generalization of the model
    * Parsimony versus black-box: number of predictors in the model

* General definition: <b> you want to find the best model/function trained on past data that will accurately predict future data. </b>

## Supervised learning

It refers to the case where your training data are labelled, or have an outcome value already measured.

NB: unsupervised learning is another field where there is not label on the data $\Rightarrow$ clustering.


Application of supervised learning: 

* Decide which movie you might want to watch next on Netflix,
* Identify the numbers in a handwritten zip code,
* Estimate gene expression level based on multiple SNPs alleles ([geno2expr](https://github.com/kevinVervier/geno2expr-ML) approach),
* Classify an individual into one of several diagnosis classes, based on multiple phenotypic measurements.

Idea: 

* Output/response/target measurement $Y$
    * In a <b><span style="color:#6495ED">regression</span></b> problem, $Y$ is a quantitative variable (e.g., price, gene expression).
    * In a <b><span style="color:#6495ED">classification</span></b> problem, $Y$ takes values in a finite set (spam/ham, digit 0-9, cancer class).
* Input/predictor/feature/covariate measurement vector $X$

Based on <b><span style="color:#6495ED">training data</span></b> which are pairs $(xi,yi)$, we are interested in:

1. accurately predicting unseen test cases, 
2. understanding which inputs affect the output,
3. assessing the quality of our predictions.



## Linear Regression

* Simple approach for supervised learning that assumes a linear dependence between $Y$ and $X$, meaning that there exists constant weights $w_i$ for each predictor, such as $Y = w_0 + w_1\times X_1 + ... + \epsilon$, where $\epsilon$ is a random noise that is not captured by the predictors.
* This approach works well when $Y$ is a numeric variable, but we will see an alternative approach for qualitative output.
* Although it may seem overly simplistic, linear regression is extremely useful both conceptually and practically.
* Parameters estimation by Residual Sum of Squares (RSS): minimizing residuals between ground truth and fitted values.

![Source: http://gerardnico.com/wiki/data_mining/residual](tutorial_slides-figure/statistics_residual.jpg)


```{r }
# Here, we will change a bit our data set, because the original Iris problem is not a regression one, but a classification problem (see later).
# Let say that we are interested in predicting the Sepal length (column 1) of a flower, given the Sepal Width.

fit <- lm(sepal_length ~ sepal_width, data = obs) # 'lm' stands for Linear Model.
summary(fit)

```

(*Quizz*) Any guess on why the Intercept/bias value is so low ?

* To test if there is a relationship between one predictor and the outcome, we could compute a t-statistic and apply a threshold on the p-value.
    * Fortunately, it is already done in `lm` $\Rightarrow$ look for the stars in the <b> pvalue </b> column (right).
* Accuracy of the fit is measured using $R^2$, also provided by `lm`. It can help for model comparison.
```{r }
# Here, we will consider the 3 variables as predictors.
fit <- lm(sepal_length ~ ., data = obs) # '.' stands for all the other variables 
summary(fit)

```

* NB: In its simplest version, linear regression does not test correlation between predictors, which could induce confusion when interpreting coefficients (here, Petal length and width).
    * In case of correlation between predictors, it is usually a good idea to consider _interactions_ between features as additional features.
    * There is also non-linear regression methods (e.g. splines or polynomial).

<b> Congrats ! You just trained your first models ! </b>
but...

<b> Meditation before break... </b>

* What about the trade-offs we discussed earlier, especially the generalization ? 
* How to evaluate your model performances on future data ?
* How to compare your model performances with other models ?

![<b>Source</b>: An Introduction to Statistical Learning with Applications in R (Fig 2.10). <b>Left</b>:
Data simulated from a function $f$ (almost linear), shown in black. Three estimates of $f$ are shown: the linear regression line (orange curve), and two smoothing spline fits (blue and green curves). <b>Right</b>: Training MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed line). Squares represent the training and test MSEs for the three fits shown in the left-hand panel.](tutorial_slides-figure/overfit.png)


## Model Selection and Regularization

In this section, we will study how to get generalizable models and how to evaluate/compare different models.

### Train/Validation/Test and Cross-validation

* As we saw in the previous picture, it is important to <b> not use the training error </b> as a performance measure (overfitting)
* The test error is the average error obtained on a new observations set, not used in the training step.
* Hold-out a subset of the training observations from the fitting process and estimate test error on it.
* Here is how to use Train/Validation/Test rule of thumb:
    * Consider 60% of your data for fitting model
    * 20% of the remaining data are used as validation set, especially if there is parameter optimization needed
    * the last 20% of the data are the test/hold-out data, only used when the final model is fitted and estimates test error
    * Main drawback: usually highly depends of the considered random split (need several repeats)
   
![Source: http://images.slideplayer.com/17/5305651/slides/slide_14.jpg](tutorial_slides-figure/train_val_test.jpg) 
    
    
```{r }

# Code for a train/validation/test split and evaluation

trainValTestLM <-function(seed = 42){
  set.seed(seed) # fix random seed for reproducible results
  # Generate random index
  idx = sample(c('train','validation','test'),nrow(obs),replace=TRUE,prob = c(0.6,0.2,0.2))
  # Sanity check
  #table(idx)

  # split X
  X.train = obs[which(idx == 'train'),]
  X.val = obs[which(idx == 'validation'),]
  X.test = obs[which(idx == 'test'),]

  # Fit model with only sepal width predictor
  fit1 <- lm(sepal_length ~ sepal_width, data = X.train)
  # Fit model with all predictors
  fit2 <- lm(sepal_length ~ ., data = X.train)

  # Evaluate both models on validation
  preds1 = predict(fit1,X.val)
  preds2 = predict(fit2,X.val)
  
  # One performance measure is the correlation between actual and predicted values:
  # Model 1
  cor1 = cor(X.val[,1],preds1)
  # Model 2
  cor2 = cor(X.val[,1],preds2)

  
  #select best model
  if( cor1 > cor2 ){
    best.model = fit1
    best = 1
  }else{
    best.model = fit2
    best = 2
  }
  
  # Do test prediction
  preds.test = predict(best.model,X.test)
  return(list('best.model' = best, 'model1' = fit1, 'model2' = fit2, 'val.perfs.1' = cor1, 'val.perfs.2' = cor2,'test.perfs' = cor(X.test[,1],preds.test)))
  
}

#Call function
results = trainValTestLM(seed = 42)
```
```{r, echo = FALSE}
cat('Model 1 performance on Validation set:',results$val.perfs.1,'\n')
cat('Model 2 performance on Validation set:',results$val.perfs.2,'\n')
cat('Model', results$best.model,'performance on Test set:',results$test.perfs,'\n')

```

Again, you may need to <b> repeat this process </b> (by changing the random seed) to assess model variability as a function of the random split (~50-100X), before applying on test.

* An alternative: $K$-fold cross-validation (e.g. $K$ = 5 or 10 or LOO)
    * randomly divide the data into $K$ equal-sized parts.
    * loop for $k$ in 1 to $K$: leave out part $k$ and fit the model to the other $K-1$ combined parts
    * obtain predictions on the $k$-th part
    * get test error estimation by combining all predictions
    * LOO stands for 'Leave-One-Out', where $K$ = `nrow(data)`.
    
![Source: http://bugra.github.io/work/notes/2014-11-22/an-introduction-to-supervised-learning-scikit-learn/](tutorial_slides-figure/cv.png) 

```{r }
# Code for a cross-validation evaluation
set.seed(42) # fix random seed for reproducible results
# Number of folds/rounds
nfolds = 10
# Generate random index
idx = sample(1:nfolds,nrow(obs),replace=TRUE) # replace = FALSE if LOO
# Sanity check
table(idx)

# Initiate vectors for storing predictions from 2 models
preds1 = rep(0,nrow(obs))
preds2 = rep(0,nrow(obs))

# Loop/Rotation over the different folds
for(fold in 1:nfolds){
  #get index of test fold
  test.idx = which(idx == fold)
  # split train/test
  X.test = obs[test.idx,]
  X.train = obs[-test.idx,]
  # Fit model with only sepal width predictor
  fit1 <- lm(sepal_length ~ sepal_width, data = X.train)
  # Fit model with all predictors
  fit2 <- lm(sepal_length ~ ., data = X.train)

  # Evaluate both models on validation
  preds1[test.idx] = predict(fit1,X.test)
  preds2[test.idx] = predict(fit2,X.test)
}

# One performance measure is the correlation between actual and predicted values:
# Model 1
cor(obs[,1],preds1)
# Model 2
cor(obs[,1],preds2)

# In terms of Mean Square Error (MSE) ~ average of residuals
# Model 1
mean((obs[,1] - preds1)^2)
# Model 2
mean((obs[,1] - preds2)^2)


```


### Model selection and Regularization

When dealing with multivariate data, it can be cumbersome to try every possible combination of features in your model.

Good news! There is more systematic ways to select the most meaningful features in your model:

* <b> Filter-based </b> selection: iteratively select the best feature subset, implying that some features are not relevant (basically, follow the stars!)
    * Backward selection starts with all the variables and remove one feature at a time,
    * Forward selection starts with empty model (intercept) and add one feature at a time.

* <b> Embedded/Shrinkage </b> approaches: feature selection during the training step, by regularizing/constraining coefficient estimates towards zero.
    * Idea: minimize a linear regression with additional constraints on the model
    $$ \min_w \mbox{RSS}(Xw,Y) + \lambda\Omega(w),$$
    where $\Omega(w)$ is a function that has higher value for $w$ not respecting the constraints,
    and $\lambda$ is a constant that controls the trade-off between the fit and the constraints.
    * Ridge regression ($L2$): keep all coefficients in the same range, but no selection ($\Omega(w) = \sum_i w_i^2$)
    * Lasso ($L1$): force some of the coefficients to be equal to 0 $\Rightarrow$ sparse model ($\Omega(w) = \sum |w_i|$)
    * Elastic-Net ($L1 + L2$): combination of ridge and Lasso, leading to sparse model and better handling correlated predictors ($\Omega(w) = (1-\alpha)\times\mbox{Ridge} + \alpha\times\mbox{Lasso}$).


![<b>Source</b>: An Introduction to Statistical Learning with Applications in R. ](tutorial_slides-figure/ridge_lasso.png)

(*Quizz*: shape of the elastic-net "ball"?)

```{r }
# Code for a cross-validation evaluation
set.seed(42) # fix random seed for reproducible results
# Number of folds/rounds
nfolds = 10
# Generate random index
idx = sample(1:nfolds,nrow(obs),replace=TRUE) # replace = FALSE if LOO

#define alpha parameter
alphas <- seq(from = 0, to = 1, by = 0.01)

#define output
mses <- rep(0,length(alphas))

#loop over alpha values
for(i in 1:length(alphas)){
  if(nfolds != nrow(obs)){
    cvfits <- cv.glmnet(x=as.matrix(obs[,-1]), y=obs[,1], alpha=alphas[i], nfolds=nfolds,foldid=idx)
  }else{
    #LOOCV
    cvfits <- cv.glmnet(x=as.matrix(obs[,-1]), y=obs[,1], alpha=alphas[i], nfolds=nfolds,grouped=FALSE)
  }
  preds.regul = predict(cvfits, newx = as.matrix(obs[,-1]), s = "lambda.min")
  mses[i] <- mean((obs[,1] - preds.regul)^2)
}
this <- data.frame(mse=mses, alpha=alphas)

plot1 <- ggplot(this, aes(x=alpha, y=mse)) +
  geom_line() +
  ylab("CV mean squared error") +
  xlab("alpha parameter") +
  ggtitle("model error of highest performing regularized elastic-net
           regression as a function of alpha parameter") + 
       #add kitchen sink
          geom_hline(aes(yintercept=mean((obs[,1] - preds2)^2)),
                       size=0.2,linetype = 2)
plot1

#get best model among alpha-range
best.alpha = alphas[tail(which(mses == min(mses)),1)]
#retrain model with best_alpha
if(nfolds != nrow(obs)){
  cvfits <- cv.glmnet(x=as.matrix(obs[,-1]), y=obs[,1], alpha=best.alpha, nfolds=nfolds,foldid=idx)
}else{
  #LOOCV
  cvfits <- cv.glmnet(x=as.matrix(obs[,-1]), y=obs[,1], alpha=best.alpha, nfolds=nfolds,grouped=FALSE)
}

# print best parameters pair
cat('Best alpha:',best.alpha,' and best lambda:',cvfits$lambda.min,'\n')
```
Given the small $\lambda$, there is little hope to observe the shrinkage effect, here.

```{r }
#get preds for the best (alpha,lambda) pair
preds.regul = predict(cvfits, newx = as.matrix(obs[,-1]), s = "lambda.min")
#Check the best model content
coef(cvfits, s = "lambda.min")
```
This model is slightly different from the Linear Model one, but different enough.

```{r }
# Simple Linear Model trained with all the features
cor(obs[,1],preds2)
cor(obs[,1],preds.regul)

# In terms of Mean Square Error (MSE)
# Linear Regression
mean((obs[,1] - preds2)^2)
# Regularized Model
mean((obs[,1] - preds.regul)^2)
```

Sometimes regularization is not largely outperforming standard Linear Regression, but it is worth to try it, because, **theoretically it will never do worse**. (*Quizz*: why ?)

---
# Session 2:  Classification methods

* Talking about classification refers to having qualitative response $Y$ that belongs to a finite set of <b>labels/classes</b>.
* Why not Liner Regression ? 
    * Example: Suppose that we are trying to predict the medical condition of a patient in the emergency room on the basis of her symptoms, where there are only three possible diagnoses: *stroke*, *drug overdose*, and *epileptic seizure*. We could consider encoding these values as a quantitative response variable, $Y$, as follows:

$$ \left\{ \begin{array}{ll} 1 & \text{ if stroke,} \\ 2 & \text{ if drug overdose,} \\ 3 & \text{ if epileptic seizure} \end{array}\right.$$

Using this coding, least squares could be used to fit a linear regression model. But unfortunately,
this coding implies an ordering on the outcomes. (*Quizz*) So why linear regression is not a good idea here ? 

So what are the alternative solutions for classification ?
There is two ways of thinking in classification: do you want to predict the class label and/or do you want a estimated probability of this class prediction ?

In the following sections, we will review standard classification algorithms with their own strategy/decision rule.

## Logistic regression
Variation of linear regression returning score between 0 and 1 (class probability).

## Support Vector Machine
maximize margin between different classes
 
## k-nearest neighbours
returns label based on neighbors labels

## decision tree
hierarchical organization of decision rules
## random forest
returns label provided by a majority vote of decision trees

* Most of the time, we are more interested into predicting the probability of a class, rather than just a class label.
* For this reason, linear regression is not a suitable option (negative probability) --> Logistic regression
* If your data contains more than two classes, there is multiple ways to extend binary classification to multiclass classification (One-versus-All, One-versus-One, ...)

<b> Meditation before concluding... </b>

Choose your data for the last session. If you have no idea on a candidate, we could pick one based on specific points you want to work on.


# Session 3: Your turn !

Here, the idea is to apply the different classification techniques to a new data set, and compare them using the evaluation framework we described in Session 1.

#Ressources

[(1) Hastie and Tibshirani Online course](http://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/)

[An Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf)

[Introduction to ML with knn applied on iris data](http://www.datacamp.com/community/tutorials/machine-learning-in-r)
